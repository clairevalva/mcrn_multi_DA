{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from resources.workspace import *","execution_count":2,"outputs":[{"output_type":"stream","text":"Initializing DAPPER...\n...Done\nPS: Turn off this message in your configuration: dpr_config.ini\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"$\n% START OF MACRO DEF\n% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN notebooks/resources/macros.py\n%\n\\newcommand{\\Reals}{\\mathbb{R}}\n\\newcommand{\\Expect}[0]{\\mathbb{E}}\n\\newcommand{\\NormDist}{\\mathcal{N}}\n%\n\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n%\n\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \n%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n\\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \n%\n\\newcommand{\\trsign}{{\\mathsf{T}}} \n\\newcommand{\\tr}{^{\\trsign}} \n\\newcommand{\\tn}[1]{#1} \n\\newcommand{\\ceq}[0]{\\mathrel{â‰”}}\n%\n\\newcommand{\\I}[0]{\\mat{I}} \n\\newcommand{\\K}[0]{\\mat{K}}\n\\newcommand{\\bP}[0]{\\mat{P}}\n\\newcommand{\\bH}[0]{\\mat{H}}\n\\newcommand{\\bF}[0]{\\mat{F}}\n\\newcommand{\\R}[0]{\\mat{R}}\n\\newcommand{\\Q}[0]{\\mat{Q}}\n\\newcommand{\\B}[0]{\\mat{B}}\n\\newcommand{\\C}[0]{\\mat{C}}\n\\newcommand{\\Ri}[0]{\\R^{-1}}\n\\newcommand{\\Bi}[0]{\\B^{-1}}\n\\newcommand{\\X}[0]{\\mat{X}}\n\\newcommand{\\A}[0]{\\mat{A}}\n\\newcommand{\\Y}[0]{\\mat{Y}}\n\\newcommand{\\E}[0]{\\mat{E}}\n\\newcommand{\\U}[0]{\\mat{U}}\n\\newcommand{\\V}[0]{\\mat{V}}\n%\n\\newcommand{\\x}[0]{\\bvec{x}}\n\\newcommand{\\y}[0]{\\bvec{y}}\n\\newcommand{\\z}[0]{\\bvec{z}}\n\\newcommand{\\q}[0]{\\bvec{q}}\n\\newcommand{\\br}[0]{\\bvec{r}}\n\\newcommand{\\bb}[0]{\\bvec{b}}\n%\n\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n%\n\\newcommand{\\D}[0]{\\mat{D}}\n\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n%\n\\newcommand{\\ones}[0]{\\bvec{1}} \n\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n%\n% END OF MACRO DEF\n$\nIn this tutorial we're going to code an EnKF implementation using numpy."},{"metadata":{},"cell_type":"markdown","source":"# The EnKF algorithm\n\nAs with the KF, the EnKF consists of the recursive application of\na forecast step and an analysis step.\nThis presentation follows the traditional template, presenting the EnKF as the \"the Monte Carlo version of the KF\nwhere the state covariance is estimated by the ensemble covariance\".\nIt is not obvious that this postulated method should work;\nindeed, it is only justified upon inspection of its properties,\ndeferred to later.\n\n<mark><font size=\"-1\">\n<b>NB:</b> \nSince we're going to focus on a single filtering cycle (at a time),\nthe subscript $k$ is dropped. Moreover, <br>\nThe superscript $f$ indicates that $\\{\\x_n^f\\}_{n=1..N}$ is the forecast (prior) ensemble.<br>\nThe superscript $a$ indicates that $\\{\\x_n^a\\}_{n=1..N}$ is the analysis (posterior) ensemble.\n</font></mark>\n\n### The forecast step\nSuppose $\\{\\x_n^a\\}_{n=1..N}$ is an iid. sample from $p(\\x_{k-1} \\mid \\y_1,\\ldots, \\y_{k-1})$, which may or may not be Gaussian.\n\nThe forecast step of the EnKF consists of a Monte Carlo simulation\nof the forecast dynamics for each $\\x_n^a$:\n$$\n\t\\forall n, \\quad \\x^f_n = \\DynMod(\\x_n^a) + \\q_n  \\, , \\\\\n$$\nwhere $\\{\\q_n\\}_{n=1..N}$ are sampled iid. from $\\NormDist(\\bvec{0},\\Q)$,\nor whatever noise model is assumed,  \nand $\\DynMod$ is the model dynamics (*any* function, i.e. the EnKF can be applied with nonlinear models).\n\nThe ensemble, $\\{\\x_n^f\\}_{n=1..N}$, is then an iid. sample from the forecast pdf,\n$p(\\x_k \\mid \\y_1,\\ldots,\\y_{k-1})$.\n\n### The analysis update step\nof the ensemble is given by:\n$$\\begin{align}\n\t\\forall n, \\quad \\x^\\tn{a}_n &= \\x_n^\\tn{f} + \\barK \\left\\{\\y - \\br_n - \\ObsMod(\\x_n^\\tn{f}) \\right\\}\n\t\\, , \\\\\n\t\\text{or,}\\quad\n\t\\E^\\tn{a} &=  \\E^\\tn{f}  + \\barK \\left\\{\\y\\ones\\tr - \\Dobs - \\ObsMod(\\E^\\tn{f})  \\right\\} \\, ,\n    \\tag{4}\n\\end{align}\n$$\nwhere the \"observation perturbations\", $\\br_n$, are sampled iid. from the observation noise model, e.g. $\\NormDist(\\bvec{0},\\R)$,  \nand form the columns of $\\Dobs$,  \nand the observation operator (again, any type of function) $\\ObsMod$ is applied column-wise to $\\E^\\tn{f}$.\n\nThe gain $\\barK$ is defined by inserting the ensemble estimates for\n * (i) $\\B \\bH\\tr$: the cross-covariance between $\\x^\\tn{f}$ and $\\ObsMod(\\x^\\tn{f})$, and\n * (ii) $\\bH \\B \\bH\\tr$: the covariance matrix of $\\ObsMod(\\x^\\tn{f})$,\n\nin the formula for $\\K$, namely eqn. (K1) of [T4](T4%20-%20Multivariate%20Kalman.ipynb).\nUsing the estimators from [T7](T7%20-%20Ensemble%20%5BMonte-Carlo%5D%20approach.ipynb) yields\n$$\\begin{align}\n\t\\barK &= \\X \\Y\\tr ( \\Y \\Y\\tr + (N{-}1) \\R )^{-1} \\, , \\tag{5a}\n\\end{align}\n$$\nwhere $\\Y \\in \\Reals^{P \\times N}$\nis the centered, *observed* ensemble.\n\nThe EnKF is summarized in the animation below.  \n<mark><font size=\"-1\">\n    For the following animation and theoretical developments, it is assumed that the observation operator is linear, represented by the matrix $\\bH$. In the animation, it is just assumed to be the identity matrix, $\\I$.\n</font></mark>"},{"metadata":{"trusted":false},"cell_type":"code","source":"EnKF_animation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exc 2:\n(a) Use the Woodbury identity (C2) of [T4](T4%20-%20Multivariate%20Kalman.ipynb) to show that eqn. (5) can also be written\n$$\\begin{align}\n\t\\barK &= \\X ( \\Y\\tr \\Ri \\Y + (N{-}1)\\I_N  )^{-1} \\Y\\tr \\Ri \\, . \\tag{5b}\n\\end{align}\n$$\n(b) What is the potential benefit?"},{"metadata":{},"cell_type":"markdown","source":"#### Exc 4:\nHow is observation operator actually treated?"},{"metadata":{},"cell_type":"markdown","source":"#### Exc 6 (a):\nConsider the ensemble averages,\n - $\\bx^\\tn{a} = \\frac{1}{N}\\sum_{n=1}^N \\x^\\tn{a}_n$, and \n - $\\bx^\\tn{f} = \\frac{1}{N}\\sum_{n=1}^N \\x^\\tn{f}_n$,\n\nand recall that the analysis step, eqn. (4), defines $\\x^\\tn{a}_n$ from $\\x^\\tn{f}_n$.\n\n\n(a) Show that, in case $\\ObsMod$ is linear (the matrix $\\bH$),\n$$\\begin{align}\n\t\\Expect \\bx^\\tn{a} &=  \\bx^\\tn{f}  + \\barK \\left\\{\\y\\ones\\tr - \\bH\\bx^\\tn{f}  \\right\\} \\, , \\tag{6}\n\\end{align}\n$$\nwhere the expectation, $\\Expect$, is taken with respect to $\\Dobs$ only (i.e. not the sampling of the forecast ensemble, $\\E^\\tn{f}$ itself).\n\nWhat does this mean?"},{"metadata":{"trusted":false},"cell_type":"code","source":"#show_answer(\"EnKF_nobias_a\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exc 6 (b)*:\nConsider the ensemble covariance matrices:\n$$\\begin{align}\n\\barB &= \\frac{1}{N-1} \\X{\\X}\\tr \\, , \\tag{7a} \\\\\\\n\\barP &= \\frac{1}{N-1} \\X^a{\\X^a}\\tr \\, . \\tag{7b}\n\\end{align}$$\n\nNow, denote the centralized observation perturbations:\n$$\\begin{align}\n\\D &= \\Dobs - \\bar{\\br}\\ones\\tr \\\\\\\n&= \\Dobs\\AN \\, . \\tag{8} \n\\end{align}$$\nNote that $\\D \\ones = \\bvec{0}$ and, with expectation over $\\Dobs$,\n$$\n\\begin{align}\n\t\\label{eqn:R_sample_cov_of_D}\n\t\\frac{1}{N-1}\\D \\D\\tr = \\R \\, , \\tag{9a} \\\\\\\n\t\\label{eqn:zero_AD_cov}\n\t\\X \\D\\tr = \\bvec{0} \\, . \\tag{9b}\n\\end{align}\n$$\nShow that\n$$\\begin{align}\n    \\barP &= [\\I_M - \\barK \\bH]\\barB \\, . \\tag{10}\n\\end{align}$$"},{"metadata":{"trusted":false},"cell_type":"code","source":"#show_answer(\"EnKF_nobias_b\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exc 6 (c)*:\nShow that, if no observation perturbations are used in eqn. (4), then $\\barP$ would be too small."},{"metadata":{"trusted":false},"cell_type":"code","source":"#show_answer(\"EnKF_without_perturbations\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Experimental setup\n\nBefore making the EnKF, we'll set up an experiment to test it with, so that you can check if you've implemented a working method or not.\n\nTo that end, we'll use the Lorenz-63 model, from [T6](T6%20-%20Dynamical%20systems,%20chaos,%20Lorenz.ipynb). The coupled ODEs are recalled here, but with some of the paremeters fixed."},{"metadata":{"trusted":false},"cell_type":"code","source":"M = 3 # ndim\n\ndef dxdt(x):\n    sig  = 10.0\n    rho  = 28.0\n    beta = 8.0/3\n    x,y,z = x\n    d     = np.zeros(3)\n    d[0]  = sig*(y - x)\n    d[1]  = rho*x - y - x*z\n    d[2]  = x*y - beta*z\n    return d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we make the forecast model $\\DynMod$ out of $\\frac{d \\x}{dt}$ such that $\\x(t+dt) = \\DynMod(\\x(t),t,dt)$. We'll make use of the \"4th order Runge-Kutta\" integrator `rk4`."},{"metadata":{"trusted":false},"cell_type":"code","source":"def Dyn(E, t0, dt):\n    \n    def step(x0):\n        return rk4(lambda t,x: dxdt(x), x0, t0, dt)\n    \n    if E.ndim == 1:\n        # Truth (single state vector) case\n        E = step(E)\n    else:\n        # Ensemble case\n        for n in range(E.shape[1]):\n            E[:,n] = step(E[:,n])\n    \n    return E\n\n\nQ_chol = zeros((M,M))\nQ      = Q_chol @ Q_chol.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice the loop over each ensemble member. For better performance, this should be vectorized, if possible. Or, if the forecast model is computationally demanding (as is typically the case in real applications), the loop should be parallellized: i.e. the forecast simulations should be distributed to seperate computers."},{"metadata":{},"cell_type":"markdown","source":"The following are the time settings that we will use"},{"metadata":{"trusted":false},"cell_type":"code","source":"dt    = 0.01           # integrational time step\ndkObs = 25             # number of steps between observations\ndtObs = dkObs*dt       # time between observations\nKObs  = 60             # total number of observations\nK     = dkObs*(KObs+1) # total number of time steps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initial conditions"},{"metadata":{"trusted":false},"cell_type":"code","source":"mu0     = array([1.509, -1.531, 25.46])\nP0_chol = eye(3)\nP0      = P0_chol @ P0_chol.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation model settings"},{"metadata":{"trusted":false},"cell_type":"code","source":"p = 3 # ndim obs\ndef Obs(E, t):\n    if E.ndim == 1: return E[:p]\n    else:           return E[:p,:]\n\nR_chol = sqrt(2)*eye(p)\nR      = R_chol @ R_chol.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate synthetic truth (`xx`) and observations (`yy`)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Init\nxx    = zeros((K+1   ,M))\nyy    = zeros((KObs+1,p))\nxx[0] = mu0 + P0_chol @ randn(M)\n\n# Loop\nfor k in range(1,K+1):\n    xx[k]  = Dyn(xx[k-1],(k-1)*dt,dt)\n    xx[k] += Q_chol @ randn(M)\n    if not k%dkObs:\n        kObs = k//dkObs-1\n        yy[kObs] = Obs(xx[k],nan) + R_chol @ randn(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EnKF implementation"},{"metadata":{},"cell_type":"markdown","source":"We will make use of `estimate_mean_and_cov` and `estimate_cross_cov` from the previous section. Paste them in below."},{"metadata":{"trusted":false},"cell_type":"code","source":"# def estimate_mean_and_cov ...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exc 8:** Complete the code below"},{"metadata":{"trusted":false},"cell_type":"code","source":"xxhat = zeros((K+1,M))\n\n# Useful linear algebra: compute B/A\ndef divide_1st_by_2nd(B,A):\n    return nla.solve(A.T,B.T).T\n\ndef my_EnKF(N):\n    # Init ensemble\n    ...\n    for k in range(1,K+1):\n        # Forecast\n        t   = k*dt\n        # use model\n        E   = ...\n        # add noise\n        E  += ...\n        if not k%dkObs:\n            # Analysis\n            y        = yy[k//dkObs-1] # current observation\n            hE       = Obs(E,t)       # obsrved ensemble\n            # Compute ensemble moments\n            BH       = ...\n            HBH      = ...\n            # Compute Kalman Gain \n            KG       = ...\n            # Generate perturbations\n            Perturb  = ...\n            # Update ensemble with KG\n            E       += \n        # Save statistics\n        xxhat[k] = mean(E,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that we only store some stats (`xxhat`). This is because in large systems, keeping the entire ensemble in memory is probably too much."},{"metadata":{"trusted":true},"cell_type":"code","source":"show_answer('EnKF v1')","execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div style=\"background-color:#dbf9ec;padding:0.5em;\"><pre><code>def my_EnKF(N):\n    E = mu0[:,None] + P0_chol @ randn((M,N))\n    for k in range(1,K+1):\n        # Forecast\n        t   = k*dt\n        E   = Dyn(E,t-dt,dt)\n        E  += Q_chol @ randn((M,N))\n        if not k%dkObs:\n            # Analysis\n            y        = yy[k//dkObs-1] # current obs\n            Eo       = Obs(E,t)\n            BH       = estimate_cross_cov(E,Eo)\n            HBH      = estimate_mean_and_cov(Eo)[1]\n            Perturb  = R_chol @ randn((p,N))\n            KG       = divide_1st_by_2nd(BH, HBH+R)\n            E       += KG @ (y[:,None] - Perturb - Eo)\n        xxhat[k] = mean(E,axis=1)\n</code></pre></div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now let's try out its capabilities"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Run assimilation\nmy_EnKF(10)\n\n# Plot\nfig, axs = plt.subplots(3,1,True)\nfor m in range(3):\n    axs[m].plot(dt*arange(K+1), xx   [:,m], 'k', label=\"Truth\")\n    axs[m].plot(dt*arange(K+1), xxhat[:,m], 'b', label=\"Estimate\")\n    if m<p:\n        axs[m].plot(dtObs*arange(1,KObs+2),yy[:,m],'g*')\n    axs[m].set_ylabel(\"Dim %d\"%m)\naxs[0].legend()\nplt.xlabel(\"Time (t)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exc 10:** The visuals of the plots are nice. But it would be good to have a summary statistic of the accuracy performance of the filter. Make a function `average_rmse(xx,xxhat)` that computes $ \\frac{1}{K+1} \\sum_{k=0}^K \\sqrt{\\frac{1}{M} \\| \\bx_k - \\x_k \\|_2^2} \\, .$"},{"metadata":{"trusted":false},"cell_type":"code","source":"def average_rmse(xx,xxhat):\n    ### INSERT ANSWER ###\n    return average\n\n# Test\naverage_rmse(xx,xxhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#show_answer('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exc 12:**\n * (a). Repeat the above expriment, but now observing only the first (0th) component of the state. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#show_answer('Repeat experiment a')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * (b). Put a `seed()` command in the right place so as to be able to recreate exactly the same results from an experiment."},{"metadata":{"trusted":false},"cell_type":"code","source":"#show_answer('Repeat experiment b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * (c). Use $N=5$, and repeat the experiments. This is quite a small ensemble size, and quite often it will yield divergence: the EnKF \"definitely loses track\" of the truth, typically because of strong nonlinearity in the forecast models, and underestimation (by $\\barP)$ of the actual errors. Repeat the experiment with different seeds until you observe in the plots that divergence has happened.\n * (d). Implement \"multiplicative inflation\" to remedy the situation; this is a factor that should spread the ensemble further apart; a simple version is to inflate the perturbations. Implement it, and tune its value to try to avoid divergence."},{"metadata":{"trusted":false},"cell_type":"code","source":"#show_answer('Repeat experiment cd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next: [Benchmarking with DAPPER](T9%20-%20Benchmarking%20with%20DAPPER.ipynb)"}],"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}